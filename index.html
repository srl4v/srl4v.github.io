<!DOCTYPE html>
<html>
<body>

<h1>SRL4V - Structured Representation Learning for Video Understanding</h1>

<p>Recent progress on spatio-temporal feature learning has pushed the state-of-the-art of action
recognition at new levels. Building upon the success of convolutional neural networks in image
recognition, currently best performing action recognition models introduce deep temporal modeling
in spatial 2D CNNs to handle time in video, with minimal overhead in parameters and computation.
They learn better features faster and with less supervision than space-time, full-3D CNNs. However,
such architectures lack the introspective means for grounded reasoning and decision making. Video
understanding relies on spatial-temporal reasoning to take place. Visual explanation methods are
being used to inspect video models after learning, but they are not yet applied to guide the learning.</p>
  
<p>This research project is set out to advance video understanding along these lines,
by lifting deep architectures for recognition and question answering by means of innate visual
explanations. Visual explanations will be built-in, and hence, interact in learning structured
representations. This way, the ’descriptor bottleneck’ of existing architectures will be eliminated
to facilitate visual grounding. The structural constraint injected via innate explanations will enable
training with less data at improved generalization, as constraints shape manifolds into the parameter
space that are hard to discover from point-wise supervisions alone.</p>
  
<p>This research is supported by <a href="https://aws.amazon.com/blogs/machine-learning/2019-q4-recipients-of-aws-machine-learning-research-awards/">Amazon AWS Machine Learning Research Awards</a> and <a href="https://www.consorzio-cini.it/index.php/en/labaiis-home/labaiis-nvaitc">NVIDIA AI Technology Center</a>.</p>
  
<h3>Publications</h3>

<p>T.M. Tai, G. Fiameni, C.K. Lee, O. Lanz: <a href="https://arxiv.org/abs/2104.08665v1">Higher Order Recurrent Space-Time Transformer</a>. arXiv:2104.08665, 2021.</p>
<p>S. Sudhakaran, A. Bulat, J.M. Perez-Rua, A. Falcon, S. Escalera, O. Lanz, B. Martinez, G. Tzimiropoulos: <a href="https://epic-kitchens.github.io/Reports/EPIC-KITCHENS-Challenges-2021-Report.pdf">SAIC_Cambridge-HuPBA-FBK Submission to the EPIC-Kitchens-100 Action
Recognition Challenge 2021</a>. EPIC-KITCHENS-100- 2021 Challenges Report, 2021.  
<p>S. Sudhakaran, S. Escalera, O. Lanz: <a href="https://doi.org/10.1109/tpami.2021.3058649">Learning to Recognize Actions on Objects in Egocentric Video with Attention Dictionaries</a>. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021.</p>
<p>S. Sudhakaran, S. Escalera, O. Lanz: <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Sudhakaran_Gate-Shift_Networks_for_Video_Action_Recognition_CVPR_2020_paper.html">Gate-Shift Networks for Video Action Recognition</a>. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.</p>
<p>A. Falcon, O. Lanz, G. Serra: <a href="https://link.springer.com/chapter/10.1007/978-3-030-66415-2_33">Data Augmentation Techniques for the Video Question Answering Task</a>. European Conference on Computer Vision (ECCV) Workshops, 2020.</p>
<p>S. Sudhakaran, S. Escalera, O. Lanz: <a href="https://arxiv.org/abs/2006.13725">FBK-HUPBA Submission to the EPIC-Kitchens Action Recognition 2020 Challenge</a>. arXiv:2006.13725, 2020.</p>
<p>S. Sudhakaran, S. Escalera, O. Lanz: <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Sudhakaran_LSTA_Long_Short-Term_Attention_for_Egocentric_Action_Recognition_CVPR_2019_paper.html">LSTA: Long Short-Term Attention for Egocentric Action Recognition</a>. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.</p>
<p>S. Sudhakaran, S. Escalera, O. Lanz: <a href="https://arxiv.org/abs/1906.08960">FBK-HUPBA Submission to the EPIC-Kitchens Action Recognition 2019 Challenge</a>. arXiv:1906.08960, 2019.</p>
<p>S. Sudhakaran, O. Lanz: <a href="http://bmvc2018.org/contents/papers/0756.pdf">Attention is All We Need: Nailing Down Object-centric Attention for Egocentric Activity Recognition</a>. British Machine Vision Conference (BMVC), 2018.</p>
  
<h3>Codes</h3>
  
<p>Results reported in papers can be reproduced with the codes published at <a href="https://github.com/swathikirans">https://github.com/swathikirans</a>.</p>

</body>
</html>
