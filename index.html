<!DOCTYPE html>
<html>
<body>

<h1>SRL4V - Structured Representation Learning for Video Understanding</h1>

<p>Recent progress on spatio-temporal feature learning has pushed the state-of-the-art of action
recognition at new levels. Building upon the success of convolutional neural networks in image
recognition, currently best performing action recognition models introduce deep temporal modeling
in spatial 2D CNNs to handle time in video, with minimal overhead in parameters and computation.
They learn better features faster and with less supervision than space-time, full-3D CNNs. However,
such architectures lack the introspective means for grounded reasoning and decision making. Video
understanding relies on spatial-temporal reasoning to take place. Visual explanation methods are
being used to inspect video models after learning, but they are not yet applied to guide the learning.</p>
  
<p>This research project is set out to advance video understanding along these lines,
by lifting deep architectures for recognition and question answering by means of innate visual
explanations. Visual explanations will be built-in, and hence, interact in learning structured
representations. This way, the ’descriptor bottleneck’ of existing architectures will be eliminated
to facilitate visual grounding. The structural constraint injected via innate explanations will enable
training with less data at improved generalization, as constraints shape manifolds into the parameter
space that are hard to discover from point-wise supervisions alone.</p>
  
<p>This research is supported by 
  <a href="https://aws.amazon.com/blogs/machine-learning/2019-q4-recipients-of-aws-machine-learning-research-awards/">Amazon AWS Machine Learning Research Awards</a> and 
  <a href="https://www.consorzio-cini.it/index.php/en/labaiis-home/labaiis-nvaitc">NVIDIA AI Technology Center</a> and
  <a href="https://www.hpc.cineca.it/services/iscra">CINECA through the Italian SuperComputing Resource Allocation - ISCRA</a>.
 </p>
  
<h3>News</h3>

<p><i style='font-size:24px' style="color:red" class='fas'>&#xf105;</i> PhD opportunity with application deadline 1st July 2022, see <a href="phd2022.html">here</a> for more information.</p>
  
<h3>Publications</h3>

<p>T.M. Tai, G. Fiameni, C.K. Lee, S. See, O. Lanz: <a href="https://arxiv.org/abs/2206.01009">Unified Recurrence Modeling for Video Action Anticipation</a>. International Conference on Pattern Recognition (ICPR), 2022.</p>
<p>A. Falcon, S. Sudhakaran, G. Serra, S. Escalera, O. Lanz: <a href="https://arxiv.org/abs/2204.13001">Relevance-based Margin for Contrastively-trained Video Retrieval Models</a>. International Conference on Multimodal Retrieval (ICMR), 2022. [<a href="https://github.com/aranciokov/RelevanceMargin-ICMR22">code</a>]</p>
<p>S. Sudhakaran, S. Escalera, O. Lanz: <a href="https://arxiv.org/abs/2203.08897">Gate-Shift-Fuse for Video Action Recognition</a>. arXiv:2203.08897, 2022.</p>
<p>A. Falcon, G. Serra, O. Lanz: <a href="https://arxiv.org/abs/2203.08688">Learning Video Retrieval Models with Relevance-Aware Online Mining</a>. International Conference on Image Analysis and Processing (ICIAP), 2022. [<a href="https://github.com/aranciokov/ranp">code</a>]</p>
<p>T.M. Tai, G. Fiameni, C.K. Lee, O. Lanz: <a href="https://arxiv.org/abs/2104.08665">Higher Order Recurrent Space-Time Transformer for Video Action Prediction</a>. arXiv:2104.08665, 2021. [<a href="https://github.com/CorcovadoMing/HORST">code</a>]</p>
<p>S. Sudhakaran, A. Bulat, J.M. Perez-Rua, A. Falcon, S. Escalera, O. Lanz, B. Martinez, G. Tzimiropoulos: <a href="https://epic-kitchens.github.io/Reports/EPIC-KITCHENS-Challenges-2021-Report.pdf">SAIC_Cambridge-HuPBA-FBK Submission to the EPIC-Kitchens-100 Action
Recognition Challenge 2021</a>. EPIC-KITCHENS-100 2021 Challenges Report, 2021.  
<p>S. Sudhakaran, S. Escalera, O. Lanz: <a href="https://doi.org/10.1109/tpami.2021.3058649">Learning to Recognize Actions on Objects in Egocentric Video with Attention Dictionaries</a>. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021.</p>
<p>S. Sudhakaran, S. Escalera, O. Lanz: <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Sudhakaran_Gate-Shift_Networks_for_Video_Action_Recognition_CVPR_2020_paper.html">Gate-Shift Networks for Video Action Recognition</a>. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [<a href="https://github.com/swathikirans/GSM">code</a>]</p>
<p>A. Falcon, O. Lanz, G. Serra: <a href="https://link.springer.com/chapter/10.1007/978-3-030-66415-2_33">Data Augmentation Techniques for the Video Question Answering Task</a>. European Conference on Computer Vision (ECCV) Workshops, 2020.</p>
<p>S. Sudhakaran, S. Escalera, O. Lanz: <a href="https://epic-kitchens.github.io/Reports/EPIC-KITCHENS-Challenges-2020-Report.pdf">FBK-HUPBA Submission to the EPIC-Kitchens Action Recognition 2020 Challenge</a>. EPIC-KITCHENS-55 2020 Challenges Report, 2020.</p>
<p>S. Sudhakaran, S. Escalera, O. Lanz: <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Sudhakaran_LSTA_Long_Short-Term_Attention_for_Egocentric_Action_Recognition_CVPR_2019_paper.html">LSTA: Long Short-Term Attention for Egocentric Action Recognition</a>. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [<a href="https://github.com/swathikirans/LSTA">code</a>]</p>
<p>S. Sudhakaran, S. Escalera, O. Lanz: <a href="https://epic-kitchens.github.io/Reports/EPIC-Kitchens-Challenges-2019-Report.pdf">FBK-HUPBA Submission to the EPIC-Kitchens Action Recognition 2019 Challenge</a>. EPIC-KITCHENS 2019 Challenges Report, 2019.</p>
<p>S. Sudhakaran, O. Lanz: <a href="http://bmvc2018.org/contents/papers/0756.pdf">Attention is All We Need: Nailing Down Object-centric Attention for Egocentric Activity Recognition</a>. British Machine Vision Conference (BMVC), 2018. [<a href="https://github.com/swathikirans/ego-rnn">code</a>]</p>

</body>
</html>
